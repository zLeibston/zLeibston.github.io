---
title: "模仿学习"
math: true
date: 2026-01-13
draft: false
cover:
    # 这是一个自动生成风景图的网站，每次刷新都不一样
    image: "https://picsum.photos/id/1/800/400" 
    alt: "随机封面"
    caption: " "
---
# 模仿学习 （imitation learning）
1. 描述：没有奖励函数，仅通过观察expert的演示数据，学习策略网络$\pi$.
    1. input:专家轨迹：$\tau =\{(O_0,a_0),(O_1,a_1),......\}$.
    2. 目标：学习策略$\pi(O):O\to a$,使其尽可能接近专家（口号：专家一定正确）
2. 若使用类似监督学习的方法，即最大化：
$$\mathbb {E}_{s_t\sim P_{data}(s_t)}(\log\pi_\theta(a_t|s_t))$$  其中，$(s_t,a_t)$为样本对，则称为“**行为克隆**”，会陷入分布迁移。即：训练用的$s_t\sim P_{data}$,而测试用的$P_{\pi_\theta}(s_t)\neq P_{data}(s_t)$,一个小错误会导致更大的错。
3. 简单的行为克隆（类似监督学习）会导致T个时间步之内的犯错的个数为$O(\epsilon T^2)$,其中$\epsilon$是监督学习后Agent看已经见过的state时犯错的概率的上界。具体如图：![](/imgs/imgs4rl-note/1.png)注意，$c(s,a)$是用来评估的，不是损失函数!
4. 一个更一般的分析：
假设：$\pi_{\theta}(a\neq \pi^{*}(s)|s)\le\epsilon$，对任意$s\sim P_{train}(s)$。注意，这里的$P_{train}$是代表了客观世界的规律，并不只局限于训练集，具体如图：![](/imgs/imgs4rl-note/2.png)
    1. 现在定义两个概率分布的L1范数或**总变差距离**：$$||p-q||x=\sum_x |p(x)-q(x)|=\int|p(x)-q(x)|dx$$而对任意两个分布p和q,有结论：$|p-q|\le2$.
    2. 推导如图：![](/imgs/imgs4rl-note/3.png)其中所有绝对值符号都是指的分布意义上的总变差距离。因此得到连续概率分布下，行为克隆的成本的上界是$O(\epsilon T^2)$.
5. 行为克隆的常见优化方法之一：不能全使用完美的标签对，要包含一些错误state对应正确action的标签对（就是数据增强），这样才有可能纠错，不至于“一步错，后面错上加错”。
6. 行为克隆的常见优化方法之二：目标条件行为克隆。如图：![](/imgs/imgs4rl-note/4.png)
7. 行为克隆的常见优化方法之三：Dagger。从训练数据入手收集更充分地数据对抗分布迁移。![](/imgs/imgs4rl-note/5.png)然后是几个natation的澄清：
   
| 符号 | 定义 | 谁产生的？ | 备注 |
| :--- | :--- | :--- | :--- |
| **$p_{\text{data}}$** | **理想国** | **专家** ($\pi^*$) | 永远是完美的，且包含所有正确路径，但只有正样本,缺少对分布外样本的改正，不够鲁棒。（我们认为，一些state是正确策略下永远无法达到的，因而不包含在$p_{theta}$里面） |
| **$p_{\theta}$** | **现实世界** | **agent** ($\pi_\theta$) | 包含了$p_{data}$没有的分布外样本。 |
| **$p_{\text{train}}$** | **教科书** | **算法决定** | **这就是你喂给 Loss 函数、用来训练的数据来源。** |

普通行为克隆中$p_{train}=p_{data}\neq p_{\theta}$。而dagger就是让$p_{train}$得到$p_{data}$所没有的分布外样本，让$p_{\theta}$更加趋近于$p_{data}$.
8. 虽说运行策略会得到一条轨迹，但我们组织$p_{train}$并不需要按轨迹来，只用以（state,action）成对的无序的字典组织就可以了
   
    